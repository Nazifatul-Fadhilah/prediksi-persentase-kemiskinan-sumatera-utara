# -*- coding: utf-8 -*-
"""Prediksi Persentase Kemiskinan LSTM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GrsJ8jvW29ZlDCbiOTiwejHE8aXfd3mE

# Importing Library
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

"""# Read data"""

data = pd.read_csv('dataPresentaseKemiskinanSumut2001-2022(1).csv',index_col='Tahun',parse_dates=True)

df = pd.read_csv('dataPresentaseKemiskinanSumut2001-2022.csv',index_col='Tahun',parse_dates=True)

df

# index_col='Tahun': Dengan parameter ini, kolom "Tahun" akan dijadikan sebagai indeks (index) dari DataFrame, sehingga data akan diindeks berdasarkan tahun.
# parse_dates=True: Parameter ini memastikan bahwa data pada kolom "Tahun" akan diuraikan sebagai tanggal, bukan sebagai string. Hal ini mempermudah manipulasi data berdasarkan tanggal.
df.index.freq='AS'
data.index.freq='AS'
# data.index.freq='AS'=> Fungsi: Kode ini mengatur frekuensi data pada indeks. Frekuensi data disetel menjadi "MS", yang berarti "Monthly Start" (Awal Bulan). Ini berarti data dianggap berfrekuensi bulanan dengan indeks pada awal setiap bulan
df

data.plot(figsize=(12,6),marker='o')

from statsmodels.tsa.seasonal import seasonal_decompose

results = seasonal_decompose(data['Persentase'])
results.plot();

len(data)

"""# Splitting and Normalization"""

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()

train = data.iloc[:18]
test = data.iloc[18:]

scaler.fit(train)
scaled_train = scaler.transform(train)
scaled_test = scaler.transform(test)

scaled_train[:10]

from keras.preprocessing.sequence import TimeseriesGenerator

# define generator
n_input = 3
n_features = 1
generator = TimeseriesGenerator(scaled_train, scaled_train, length=n_input, batch_size=1)

X,y = generator[0]
print(f'Given the Array: \n{X.flatten()}')
print(f'Predict this y: \n {y}')

X.shape

from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM

"""# Define Model"""

# define model
model = Sequential()
model.add(LSTM(64, activation='relu', input_shape=(n_input, n_features)))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mse')

model.summary()

# fit model
model.fit(generator,epochs=150)

loss_per_epoch = model.history.history['loss']
plt.plot(range(len(loss_per_epoch)),loss_per_epoch)

# Nilai Error Modelling data
average_loss_per_epoch = np.mean(loss_per_epoch)
print('\n',average_loss_per_epoch)

"""# Pengujian"""

last_train_batch = scaled_train[-3:]

last_train_batch = last_train_batch.reshape((1, n_input, n_features))

model.predict(last_train_batch)

scaled_test[0]

test_predictions = []

first_eval_batch = scaled_train[-n_input:]
current_batch = first_eval_batch.reshape((1, n_input, n_features))

for i in range(len(test)):

    # get the prediction value for the first batch
    current_pred = model.predict(current_batch)[0]

    # append the prediction into the array
    test_predictions.append(current_pred)

    # use the prediction to update the batch and remove the first value
    current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1)

test_predictions

test

true_predictions = scaler.inverse_transform(test_predictions)

test['Predictions'] = true_predictions

test1 = test.drop("2023-01-01",axis="rows")

test2 = test.drop(test.tail(3).index)

test1.plot(figsize=(14,5))

from sklearn.metrics import mean_squared_error
from math import sqrt
mse= mean_squared_error(test['Persentase'],test['Predictions'])
rmse=sqrt(mean_squared_error(test['Persentase'],test['Predictions']))
print(rmse)
